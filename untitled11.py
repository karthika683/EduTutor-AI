# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zegfiiKVQ2pc8Lhg9kabuYrIu8xGcVp9
"""

!pip install transformers torch gradio -q

# Educational AI Application using IBM Granite Model
# Run this in Google Colab
!pip install transformers torch gradio -q

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    inputs = {key: val.to(model.device) for key, val in inputs.items()}
    outputs = model.generate(**inputs, max_length=max_length)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Gradio Interface
with gr.Blocks() as app:
    with gr.Tab("Concept Explanation"):
        concept_input = gr.Textbox(label="Enter a concept", placeholder="e.g., machine learning")
        explain_btn = gr.Button("Explain")
        explanation_output = gr.Textbox(label="Explanation", lines=10)

        explain_btn.click(generate_response, inputs=concept_input, outputs=explanation_output)

    with gr.Tab("Quiz Generator"):
        quiz_input = gr.Textbox(label="Enter a topic", placeholder="e.g., physics")
        quiz_btn = gr.Button("Generate Quiz")
        quiz_output = gr.Textbox(label="Quiz Questions", lines=15)

        quiz_btn.click(generate_response, inputs=quiz_input, outputs=quiz_output)

app.launch(share=True)